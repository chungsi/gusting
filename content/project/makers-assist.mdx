---
title: Maker's Assist
subtitle: an accessibility app design for the mobility impaired
heroImage: './../assets/puff.png'
heroImagePos: top

category: design
tags:
  - user experience
  - interface design

gallery:
  - ./../assets/makersAssist/journey_map_sm.jpg
  - ./../assets/makersAssist/journey_map_lg.jpg
  - ./../assets/makersAssist/proposals.jpg
  - ./../assets/makersAssist/testing_in_person.jpg
  - ./../assets/makersAssist/onboarding_actions.jpg
  - ./../assets/makersAssist/swipe_zoom.jpg
  - ./../assets/makersAssist/customise.jpg

publish: true
feature: true
---

**timeline**: 2.5 months  
**team size**: five people

## My Roles

Visual design, illustration, and UI  
UX: User research, interviewing our audiences, user testing  
Videography: Camera, editing, sound design


## Pitch

For the Makers Assist App, we partnered with the Neil Squire Society (NSS) to help them design a UI/UX for their newly created LipSync device, which helps quadriplegics navigate their mobile devices with the control of their mouth only. However, the LipSync could not press hardware buttons nor perform complex finger gestures (ie. swiping). So together with the Neil Squire Society, we determined that the best solution is a UI overlay that would convert these issues into simple click buttons.

The goal was to create an effective user experience that could make mobile devices as easy to use for quadriplegics as they are for any able-bodied person. Through the process of user research, idea proposal, and user testing, we created the final UI overlay design that is most intuitive for quadriplegics.

Below is the final concept pitch video.

<!-- TODO: Fix the iframe and squishing on mobile... -->
<div className='w-full'>
<iframe title="vimeo-player" src="https://player.vimeo.com/video/211353188" width="800" height="450" frameborder="0" allowfullscreen></iframe>
</div>

## Process
\# [research](#research)  
\# [proposals](#proposals)  
\# [initial testing & struggles](#initial-testing--struggles)  
\# [final design](#final-design)  
\# [reflections](#reflections)  

### Research

The first step after meeting with Neil Squire Society was to conduct user research. We looked at solutions made by competing companies and met with users to learn about their habits and routines. In particular, we interviewed people with limited or no hand use at a LipSync workshop.

We then created a journey map after collecting our research to further analyse the process and pain points of three seemingly trivial tasks to the physically able:

* Typing and scrolling while using Google Search;
* Swiping to answer a phone call; and
* Pinch-zooming and scrolling while using Google Maps.

We found these tasks are extremely difficult for those with motor disabilities, and thus, aimed our proposals to address them.

<MdxImage shadow gatsbyImageData={props.gallery.image1} link={props.gallery.image2} />


### Proposals

Our research showed that users wanted to modify the environment to their own needs. After a process of ideation, we decided on a two-level menu system to reduce movement-distance with the mouse. A circle-overlay opens into a menu of hardware tasks while a quick-access shelf at the bottom of the screen has frequently used apps or actions. User customization would be added in the initial design.

Below are the initial sketches we built-off of, combined, and refined to create our first prototype.

<MdxImage gatsbyImageData={props.gallery.image3} />


### Initial Testing & Struggles

During the initial design of the UI overlay, we created a fully interactive Axure mockup to simulate the mobile devices we were using. This mockup was embedded into an android application for users to test our proposed system.

Quadriplegics then tested our mockup with their own LipSync hardware device. It was here where we discovered the inherent problems with the LipSync, and the challenges in designing for a device that is still in development. While we didn't receive the feedback we expected, we did find that users struggled to understand how the LipSync worked. This gave us an opportunity to embed this information into the UI, and hopefully have users understand the LipSync before they incorporate it to their mobile device.

<MdxImage shadow gatsbyImageData={props.gallery.image4} />


## Final Design

We compiled our final concept pitch in a video, explaining the features we designed and the scenarios we explored in our journey map. Below is an overview of the features explained.

### Onboarding

The onboarding to LipSync and our Maker's Assist application was heavily influenced by our user testing. Users learn about the modes that include the actions of: swiping, drag & drop, scroll, zoom, and screenshot.

<MdxImage shadow gatsbyImageData={props.gallery.image5} />

### Interactions

Example interactions of the Maker's Assist app below show swiping and zooming, which make use of the bottom quick menu for actions and the circle-overlap to remind users what interaction mode they are currently in.

<MdxImage shadow gatsbyImageData={props.gallery.image6} />

### Customisation

Users can set their custom actions for either menu easily.

<MdxImage shadow gatsbyImageData={props.gallery.image7} />


## Reflections

Ultimately, our concentration was not on building features into the application. Rather, we worked to streamline the onboarding and customization of the UI and the LipSync device. With the focus on user experience, we were able to learn a lot about understanding the users while working with clients. This made the implementation of the system easy for the developer at NSS, so that he could concentrate on the code rather than the process of the project.